# STEP 1 CODE

# Import the packages needed for this analysis
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import wavfile

# We will now read the audio file and determine the audio signal and sampling frequency 
# Give the path of the file
freq_sample, sig_audio = wavfile.read("Welcome.wav")

# Output the parameters: Signal Data Type, Sampling Frequency and Duration
print('\nShape of Signal:', sig_audio.shape)
print('Signal Datatype:', sig_audio.dtype)
print('Signal duration:', round(sig_audio.shape[0] / float(freq_sample), 2), 'seconds')

# Normalize the signal values
pow_audio_signal = sig_audio / np.power(2, 15)

# We shall now extract the first 100 values from the signal 
pow_audio_signal = pow_audio_signal [:100]
time_axis = 1000 * np.arange(0, len(pow_audio_signal), 1) / float(freq_sample)

# Visualize the signal
plt.plot(time_axis, pow_audio_signal, color='blue')
plt.xlabel('Time (milliseconds)')
plt.ylabel('Amplitude')
plt.title('Input audio signal')
plt.show()


# STEP 2 CODE

# Characterization of the signal from the input file
# We will be using Fourier Transforms to convert the signals to a frequency domain distribution

import numpy as np
import matplotlib.pyplot as plt
from scipy.io import wavfile

freq_sample, sig_audio = wavfile.read("Welcome.wav")

print('\nShape of the Signal:', sig_audio.shape)
print('Signal Datatype:', sig_audio.dtype)
print('Signal duration:', round(sig_audio.shape[0] / float(freq_sample), 2), 'seconds')

sig_audio = sig_audio / np.power(2, 15)

# Extracting the length and the half-length of the signal to input to the foruier transform
sig_length = len(sig_audio)
half_length = np.ceil((sig_length + 1) / 2.0).astype(np.int)

# We will now be using the Fourier Transform to form the frequency domain of the signal
signal_freq = np.fft.fft(sig_audio)
# Normalize the frequency domain and square it 
signal_freq = abs(signal_freq[0:half_length]) / sig_length
signal_freq **= 2

transform_len = len(signal_freq)
# The Fourier transformed signal now needs to be adjusted for both even and odd cases

if sig_length % 2:
    signal_freq[1:transform_len] *= 2
else:
    signal_freq[1:transform_len-1] *= 2

# Extract the signal's strength in decibels (dB)
exp_signal = 10 * np.log10(signal_freq)

x_axis = np.arange(0, half_length, 1) * (freq_sample / sig_length) / 1000.0

plt.figure()
plt.plot(x_axis, exp_signal, color='green', linewidth=1)
plt.xlabel('Frequency Representation (kHz)')
plt.ylabel('Power of Signal (dB)')
plt.show()

# CODE SNIPPET 3

import numpy as np
import matplotlib.pyplot as plt
from scipy.io.wavfile import write

# Specify the output file where this data needs to be stored
output_file = 'generated_signal_audio.wav'

# Duration in seconds, Sampling Frequency in Hz
sig_duration = 8 
sig_frequency_sampling = 74100 
sig_frequency_tone = 802
sig_min_val = -5 * np.pi
sig_max_val = 5 * np.pi

# Generating the audio signal
temp_signal = np.linspace(sig_min_val, sig_max_val, sig_duration * sig_frequency_sampling)
temp_audio_signal = np.sin(2 * np.pi * sig_frequency_tone * temp_signal)

# The write() function creates a frequency based sound signal and writes it to the created file
write(output_file, sig_frequency_sampling, temp_audio_signal)

sig_audio = temp_audio_signal[:100]
def_time_axis = 1000 * np.arange(0, len(sig_audio), 1) / float(sig_frequency_sampling)

plt.plot(def_time_axis, sig_audio, color='green')
plt.xlabel('Time (milliseconds)')
plt.ylabel('Sound Amplitude')
plt.title('Audio Signal Generation')
plt.show()

# CODE SNIPPET 4

pip install python_speech_features

# Import the necessary pacakges
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import wavfile
from python_speech_features import mfcc, logfbank

sampling_freq, sig_audio = wavfile.read("Welcome.wav")

# We will now be taking the first 15000 samples from the signal for analysis
sig_audio = sig_audio[:15000]


# Using MFCC to extract features from the signal
mfcc_feat = mfcc(sig_audio, sampling_freq)

print('\nMFCC Parameters\nWindow Count =', mfcc_feat.shape[0])
print('Individual Feature Length =', mfcc_feat.shape[1])

mfcc_feat = mfcc_feat.T
plt.matshow(mfcc_feat)
plt.title('MFCC Features')

# Generating filter bank features
fb_feat = logfbank(sig_audio, sampling_freq)

print('\nFilter bank\nWindow Count =', fb_feat.shape[0])
print('Individual Feature Length =', fb_feat.shape[1])

fb_feat = fb_feat.T
plt.matshow(fb_feat)
plt.title('Features from Filter bank')
plt.show()

# CODE SNIPPET 5


pip install SpeechRecognition

pip install pipwin

# https://anaconda.org/anaconda/pyaudio
# https://www.lfd.uci.edu/~gohlke/pythonlibs/#pyaudio
# Run in the Anaconda Terminal CMD: conda install -c anaconda pyaudio
# Microsoft Visual C++ 14.0 or greater is required. Get it with "Microsoft C++ Build Tools": https://visualstudio.microsoft.com/visual-cpp-build-tools/

pip install pyaudio

import speech_recognition as speech_recog

# Creating a recording object to store input
rec = speech_recog.Recognizer()

# Importing the microphone class to check availabiity of microphones
mic_test = speech_recog.Microphone()

# List the available microphones
speech_recog.Microphone.list_microphone_names()

# We will now directly use the microphone module to capture voice input
# Specifying the second microphone to be used for a duration of 3 seconds
# The algorithm will also adjust given input and clear it of any ambient noise
with speech_recog.Microphone(device_index=1) as source: 
    rec.adjust_for_ambient_noise(source, duration=3)
    print("Reach the Microphone and say something!")
    audio = rec.listen(source)

try:
    print("I think you said: \n" + rec.recognize_google(audio))
except Exception as e:
    print(e)
